{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_assignment_2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blainerothrock/nlp-group-2/blob/master/nlp_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5suI3fCFjhO",
        "colab_type": "text"
      },
      "source": [
        "# NLP Assignment 2 (Bengio and other Neural Language Models)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD4GX_ezyPag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "print(tf.__version__)\n",
        "\n",
        "from google.colab import drive, files \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "import math\n",
        "import typing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYLEc_SQLTmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnA8lII6Al51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blaine_data_path = '/Users/blaine/Google Drive File Stream/My Drive/Winter20/nlp/nlp_group2/data'\n",
        "data_path = 'drive/My Drive/Winter20/nlp/nlp_group2/data'\n",
        "grant_data_path = 'drive/My Drive/nlp_group2/data'\n",
        "sundar_data_path = 'drive/My Drive/nlp_group2/data'\n",
        "z_data_path = 'drive/My Drive/nlp_group2/data'\n",
        "sundar_local_path = '~/Workspaces/Q2/NLP/data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hux8jIA0OWWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = data_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X8tKzUSzAlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(os.listdir(data_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWiLJQNrE72M",
        "colab_type": "text"
      },
      "source": [
        "## Task 1\n",
        "Split train corpus with `batch_size=30` and `window=5` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6iLwqmo_Do1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "int_train = pickle.load(open(os.path.join(data_path, 'group2.int_train.p'), 'rb'))\n",
        "int_validation = pickle.load(open(os.path.join(data_path, 'group2.int_valid.p'), 'rb'))\n",
        "int_test = pickle.load(open(os.path.join(data_path, 'group2.int_test.p'), 'rb'))\n",
        "train = pickle.load(open(os.path.join(data_path, 'group2.train.p'), 'rb'))\n",
        "vocab_dict = pickle.load(open(os.path.join(data_path, 'group2.vocab_dict.p'), 'rb'))\n",
        "# int_train = [vocab_dict[w] for w in train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pw_br8mAJ58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(int_train[:10], '\\n', train[:10])\n",
        "\n",
        "print(\"vocab len: %i\" % len(vocab_dict))\n",
        "print(\"int rep: %s\" % len(int_train))\n",
        "print(\"train token: %s\" % len(train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFbxb_nIAQHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch the train integer representations\n",
        "def gen_batches(context_size, num_batches, data):\n",
        "  num_data = len(data)\n",
        "\n",
        "  # removing remainder tokens\n",
        "  remainder = num_data % math.floor(num_data/num_batches)\n",
        "  print(remainder)\n",
        "  data = data[:num_data - remainder]\n",
        "  num_data = len(data)\n",
        "\n",
        "  # batches = np.array_split(data, math.floor(num_data)/batch_size)\n",
        "  batches = np.split(np.array(data),num_batches,axis=0)\n",
        "  return batches\n",
        "\n",
        "batches_words = gen_batches(5, 30, train)\n",
        "batches_int = gen_batches(5, 30, int_train)\n",
        "# batches_embeddings = gen_batches(5, 30, embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg3b1XELNCYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_seq(batch, window, seq_idx):\n",
        "  input_tokens = batch[seq_idx:seq_idx+window]\n",
        "  target_token = batch[seq_idx+window]\n",
        "\n",
        "  print(\"input : \", input_tokens)\n",
        "  print(\"target: [\", target_token, \"]\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkBM381LNChw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('--- batch 01 ---')\n",
        "print_seq(batches_words[0], 5, 0)\n",
        "print_seq(batches_words[0], 5, 1)\n",
        "print_seq(batches_words[0], 5, 2)\n",
        "\n",
        "print('-- batch 02 --')\n",
        "print_seq(batches_words[1], 5, 0)\n",
        "print_seq(batches_words[1], 5, 1)\n",
        "print_seq(batches_words[1], 5, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6gNu8xBSizG",
        "colab_type": "text"
      },
      "source": [
        "## Task 2: Bengio Style Feedforward network language model\n",
        "- TensorFlow version: `2.1.0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWCFC4p1RMPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BengioParams():\n",
        "\n",
        "  def __init__(self, vocab_dict):\n",
        "    self.context_window = 5\n",
        "    self.num_batches = 30\n",
        "\n",
        "    self.vocab_len = len(vocab_dict)\n",
        "    \n",
        "    self.hidden_units = 50\n",
        "    self.embeddings_dim = 60\n",
        "    self.num_epochs = 20\n",
        "\n",
        "    self.learning_rate = 0.5\n",
        "\n",
        "    self.gpu_mem = 0.25\n",
        "    \n",
        "    self.tf_precision = tf.float32\n",
        "    self.np_precision = np.float32\n",
        "\n",
        "    self.init_scale = 0.5\n",
        "    self.max_grad = 10.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj6d3vk0U0lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BengioModel():\n",
        "  \"\"\"\n",
        "  Class implements Bengio NN model with Tensorflow accoring to the function:\n",
        "    y = b + Wx + Utanh(d + Hx)\n",
        "  \n",
        "  and \n",
        "    cost = softmax_cross_entropy?\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, params):\n",
        "\n",
        "    self.Y = tf.placeholder(\n",
        "        dtype=params.tf_precision,\n",
        "        shape=(params.num_batches, params.vocab_len),\n",
        "        name=\"Y\"\n",
        "    )\n",
        "    \n",
        "    self.X = tf.placeholder(\n",
        "        tf.int32, \n",
        "        shape=(params.num_batches, params.context_window),\n",
        "        name=\"X\"\n",
        "    )\n",
        "\n",
        "    # embeddings\n",
        "    self.C = tf.Variable(\n",
        "        tf.truncated_normal(\n",
        "            shape=(params.vocab_len, params.embeddings_dim),\n",
        "            mean=-1,\n",
        "            stddev=-1\n",
        "        ),\n",
        "        dtype=params.tf_precision,\n",
        "        name=\"C\"\n",
        "    )\n",
        "\n",
        "    self.W = tf.Variable(\n",
        "        tf.random_normal(\n",
        "            shape=(params.vocab_len, params.context_window * params.embeddings_dim)\n",
        "        ),\n",
        "        name=\"W\",\n",
        "        dtype=params.tf_precision\n",
        "    )\n",
        "    \n",
        "    self.H = tf.Variable(\n",
        "        tf.random_normal(\n",
        "            shape=(params.hidden_units, params.context_window * params.embeddings_dim)\n",
        "        ),\n",
        "        name=\"H\",\n",
        "        dtype=params.tf_precision\n",
        "    )\n",
        "\n",
        "    self.d = tf.Variable(\n",
        "        tf.random_normal(\n",
        "            shape=(params.hidden_units,)\n",
        "        ),\n",
        "        name=\"d\",\n",
        "        dtype=params.tf_precision\n",
        "    )\n",
        "\n",
        "    self.U = tf.Variable(\n",
        "        tf.random_normal(\n",
        "            (params.vocab_len, params.hidden_units)\n",
        "        ),\n",
        "        name=\"U\",\n",
        "        dtype=params.tf_precision\n",
        "    )\n",
        "\n",
        "    self.b = tf.Variable(\n",
        "        tf.random_normal(\n",
        "            shape=(params.vocab_len, )\n",
        "        ),\n",
        "        name=\"b\",\n",
        "        dtype=params.tf_precision\n",
        "    )\n",
        "\n",
        "    with tf.name_scope(\"Projection_Layer\"):\n",
        "      x = tf.nn.embedding_lookup(self.C, self.X)\n",
        "      x = tf.reshape(\n",
        "          x,\n",
        "          shape=(params.num_batches, params.context_window * params.embeddings_dim)\n",
        "      )\n",
        "\n",
        "    with tf.name_scope(\"Hidden_Layer\"):\n",
        "      Hx = tf.matmul(x, tf.transpose(self.H))\n",
        "      a = tf.nn.tanh(tf.add(Hx, self.d))\n",
        "\n",
        "    with tf.name_scope(\"Output_Layer\"):\n",
        "      Ua = tf.matmul(a, tf.transpose(self.U))\n",
        "      Wx = tf.matmul(x, tf.transpose(self.W))\n",
        "      Y_hat = tf.add(self.b, tf.add(Wx, Ua)) \n",
        "\n",
        "    with tf.name_scope(\"Cost\"):\n",
        "     self.cost = tf.reduce_mean( \n",
        "        tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "            labels=self.Y,\n",
        "            logits=Y_hat\n",
        "        )\n",
        "      )\n",
        "     self.perplexity = tf.exp(self.cost)\n",
        "\n",
        "    self.optimizer = tf.train.GradientDescentOptimizer(params.learning_rate).minimize(self.cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is2xf4mESMVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spiltInputTarget(batch, win_idx, params):\n",
        "  _x = batch[win_idx:win_idx + params.context_window]\n",
        "  _y = np.zeros(params.vocab_len)\n",
        "  _y[batch[win_idx + params.context_window]] = 1\n",
        "  return _x, _y\n",
        "\n",
        "\n",
        "def run(model, params, batches_train_int, batches_validation_int, batches_test_int):\n",
        "\n",
        "  perplexity_history = []\n",
        "  cost_history = []\n",
        "\n",
        "  val_perplexity_history = []\n",
        "  val_cost_history = []\n",
        "\n",
        "  test_perplexity_history = []\n",
        "  test_cost_history = []\n",
        "  \n",
        "\n",
        "  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)\n",
        "  with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True)) as sess:\n",
        "    initializer = tf.global_variables_initializer()\n",
        "    initializer.run()\n",
        "    step = 0\n",
        "    for epoch in range(params.num_epochs):\n",
        "      # run model with 30 batches for a window size\n",
        "      # for idx in total number of window sizes\n",
        "        # for batch in batch_int\n",
        "      win_idx = 0\n",
        "      while win_idx < (len(batches_train_int[0]) - params.context_window - 1):\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for batch in batches_train_int:\n",
        "          _x, _y = spiltInputTarget(batch, win_idx, params)\n",
        "          batch_x.append(_x)\n",
        "          batch_y.append(_y)\n",
        "        \n",
        "        cost, perplexity, _ = sess.run(\n",
        "            [model.cost, model.perplexity, model.optimizer], \n",
        "            feed_dict={ model.X:batch_x, model.Y:batch_y }\n",
        "        )\n",
        "\n",
        "        # calculate validation & test preplexity after each epoch\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "          print(\"train: step {}, cost: {}, perplexity: {}\".format(step, cost, perplexity))\n",
        "          perplexity_history.append(perplexity)\n",
        "          cost_history.append(cost)\n",
        "        \n",
        "        step+=1\n",
        "        win_idx+=1\n",
        "\n",
        "      # validation\n",
        "      win_idx = 0\n",
        "      val_perplexity = 0\n",
        "      val_cost = 0\n",
        "      while win_idx < (len(batches_validation_int[0]) - params.context_window - 1):\n",
        "        val_batch_x = []\n",
        "        val_batch_y = []\n",
        "        for batch in batches_validation_int:\n",
        "          _x, _y = spiltInputTarget(batch, win_idx, params)\n",
        "          val_batch_x.append(_x)\n",
        "          val_batch_y.append(_y)\n",
        "\n",
        "        cost, perplexity = sess.run(\n",
        "            [model.cost, model.perplexity], \n",
        "            feed_dict={ model.X:val_batch_x, model.Y:val_batch_y }\n",
        "        )\n",
        "        val_perplexity = perplexity\n",
        "        val_cost = cost\n",
        "        \n",
        "        win_idx+=1\n",
        "      \n",
        "      val_perplexity_history.append(val_perplexity)\n",
        "      val_cost_history.append(val_cost)\n",
        "      print(\"validation: epoch {}, cost: {}, perplexity: {}\".format(epoch, val_cost, val_perplexity))\n",
        "\n",
        "      # test\n",
        "      win_idx = 0\n",
        "      test_perplexity = 0\n",
        "      test_cost = 0\n",
        "      while win_idx < (len(batches_test_int[0]) - params.context_window - 1):\n",
        "        test_batch_x = []\n",
        "        test_batch_y = []\n",
        "        for batch in batches_test_int:\n",
        "          _x, _y = spiltInputTarget(batch, win_idx, params)\n",
        "          test_batch_x.append(_x)\n",
        "          test_batch_y.append(_y)\n",
        "\n",
        "        cost, perplexity = sess.run(\n",
        "            [model.cost, model.perplexity], \n",
        "            feed_dict={ model.X:test_batch_x, model.Y:test_batch_y }\n",
        "        )\n",
        "        test_perplexity = perplexity\n",
        "        test_cost = cost\n",
        "        \n",
        "        win_idx+=1\n",
        "      \n",
        "      test_perplexity_history.append(test_perplexity)\n",
        "      test_cost_history.append(test_cost)\n",
        "      print(\"test: epoch {}, cost: {}, perplexity: {}\".format(epoch, test_cost, test_perplexity_history))\n",
        "\n",
        "  return perplexity_history, cost_history, val_perplexity_history, val_cost_history, test_perplexity_history, test_cost_history\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWBxECjWWXn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# brown\n",
        "\n",
        "# read the brown corpus text file\n",
        "with open(os.path.join(data_path, 'brown_tokenized.txt'), 'r') as f:\n",
        "  brown_tokens_all = f.readline().split(\" \")\n",
        "\n",
        "# remove tokens with less than 3 freq\n",
        "brown_tok_freq = {}\n",
        "for idx, tok in enumerate(brown_tokens_all):\n",
        "  brown_tok_freq[tok] = brown_tok_freq.get(tok, 0) + 1\n",
        "\n",
        "brown_tokens_all = [tok for tok in filter(lambda x: brown_tok_freq[x] >= 3, brown_tokens_all)]  \n",
        "\n",
        "# create vocab\n",
        "brown_vocab = set([tok for tok in brown_tokens_all])\n",
        "\n",
        "# create train, validation, test\n",
        "brown_train = brown_tokens_all[:800000]\n",
        "brown_validation = brown_tokens_all[800000:1000000]\n",
        "brown_test = brown_tokens_all[1000000:]\n",
        "\n",
        "print(\"size of brown vocab: %i\" % len(brown_vocab))\n",
        "\n",
        "# integer representation\n",
        "brown_vocab_dict = {}\n",
        "for i, v in enumerate(brown_vocab):\n",
        "    brown_vocab_dict[v] = i\n",
        "\n",
        "brown_train_int = [brown_vocab_dict[tok] for tok in brown_train]\n",
        "brown_validation_int = [brown_vocab_dict[tok] for tok in brown_validation]\n",
        "brown_test_int = [brown_vocab_dict[tok] for tok in brown_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7owo5_W9Wb7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train on brown\n",
        "# brown_train_int\n",
        "\n",
        "brown_params = BengioParams(brown_vocab_dict)\n",
        "brown_model = BengioModel(params=brown_params)\n",
        "\n",
        "brown_train_batches_int = gen_batches(5, 30, brown_train_int)\n",
        "brown_val_batches_int = gen_batches(5, 30, brown_validation_int)\n",
        "brown_test_batches_int = gen_batches(5, 30, brown_test_int)\n",
        "\n",
        "brown_perplexity_history,  brown_cost_history, brown_val_perplexity_history, brown_val_cost_history, brown_test_perplexity_history, brown_test_cost_history = run(brown_model, brown_params, brown_train_batches_int, brown_val_batches_int, brown_test_batches_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1z2jH1dsIE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(brown_perplexity_history, open(os.path.join(data_path, 'brown_perplexity_history.p'), 'wb'))\n",
        "pickle.dump(brown_cost_history, open(os.path.join(data_path, 'brown_cost_history.p'), 'wb'))\n",
        "pickle.dump(brown_val_perplexity_history, open(os.path.join(data_path, 'brown_val_perplexity_history.p'), 'wb'))\n",
        "pickle.dump(brown_val_cost_history, open(os.path.join(data_path, 'brown_val_cost_history.p'), 'wb'))\n",
        "pickle.dump(brown_test_perplexity_history, open(os.path.join(data_path, 'brown_test_perplexity_history.p'), 'wb'))\n",
        "pickle.dump(brown_test_cost_history, open(os.path.join(data_path, 'brown_test_cost_history.p'), 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPOb8kVXAzRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train our model\n",
        "class BengioModel2():\n",
        "  \"\"\"\n",
        "  Class implements Bengio NN model with Tensorflow accoring to the function:\n",
        "    y = b + Utanh(d + Hx)\n",
        "  \n",
        "  and \n",
        "    cost = softmax_cross_entropy?\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, params):\n",
        "\n",
        "    self.Y = tf.placeholder(\n",
        "        dtype=params.tf_precision,\n",
        "        shape=(params.num_batches, params.vocab_len),\n",
        "        name=\"Y\"\n",
        "    )\n",
        "    \n",
        "    self.X = tf.placeholder(\n",
        "        tf.int32, \n",
        "        shape=(params.num_batches, params.context_window),\n",
        "        name=\"X\"\n",
        "    )\n",
        "\n",
        "    # embeddings\n",
        "    self.C = tf.Variable(\n",
        "        tf.truncated_normal(\n",
        "            shape=(params.vocab_len, params.embeddings_dim),\n",
        "            mean=-1,\n",
        "            stddev=-1\n",
        "        ),\n",
        "        dtype=params.tf_precision,\n",
        "        name=\"C\"\n",
        "    )\n",
        "\n",
        "    # self.W = tf.Variable(\n",
        "    #     tf.random_normal(\n",
        "    #         shape=(params.vocab_len, params.context_window * params.embeddings_dim)\n",
        "    #     ),\n",
        "    #     name=\"W\",\n",
        "    #     dtype=params.tf_precision\n",
        "    # )\n",
        "    \n",
        "    self.H = tf.Variable(\n",
        "        tf.random_normal(\n",
        "            shape=(params.hidden_units, params.context_window * params.embeddings_dim)\n",
        "        ),\n",
        "        name=\"H\",\n",
        "        dtype=params.tf_precision\n",
        "    )\n",
        "\n",
        "    self.d = tf.Variable(\n",
        "        tf.random_normal(\n",
        "            shape=(params.hidden_units,)\n",
        "        ),\n",
        "        name=\"d\",\n",
        "        dtype=params.tf_precision\n",
        "    )\n",
        "\n",
        "    self.U = tf.Variable(\n",
        "        tf.random_normal(\n",
        "            (params.vocab_len, params.hidden_units)\n",
        "        ),\n",
        "        name=\"U\",\n",
        "        dtype=params.tf_precision\n",
        "    )\n",
        "\n",
        "    self.b = tf.Variable(\n",
        "        tf.random_normal(\n",
        "            shape=(params.vocab_len, )\n",
        "        ),\n",
        "        name=\"b\",\n",
        "        dtype=params.tf_precision\n",
        "    )\n",
        "\n",
        "    with tf.name_scope(\"Projection_Layer\"):\n",
        "      x = tf.nn.embedding_lookup(self.C, self.X)\n",
        "      x = tf.reshape(\n",
        "          x,\n",
        "          shape=(params.num_batches, params.context_window * params.embeddings_dim)\n",
        "      )\n",
        "\n",
        "    with tf.name_scope(\"Hidden_Layer\"):\n",
        "      Hx = tf.matmul(x, tf.transpose(self.H))\n",
        "      a = tf.nn.tanh(tf.add(Hx, self.d))\n",
        "\n",
        "    with tf.name_scope(\"Output_Layer\"):\n",
        "      Ua = tf.matmul(a, tf.transpose(self.U))\n",
        "      # Wx = tf.matmul(x, tf.transpose(self.W))\n",
        "      Y_hat = tf.add(self.b, Ua) \n",
        "\n",
        "    with tf.name_scope(\"Cost\"):\n",
        "     self.cost = tf.reduce_mean( \n",
        "        tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "            labels=self.Y,\n",
        "            logits=Y_hat\n",
        "        )\n",
        "      )\n",
        "     self.perplexity = tf.exp(self.cost)\n",
        "\n",
        "    self.optimizer = tf.train.GradientDescentOptimizer(params.learning_rate).minimize(self.cost)\n",
        "\n",
        "int_train = pickle.load(open(os.path.join(data_path, 'group2.int_train.p'), 'rb'))\n",
        "int_validation = pickle.load(open(os.path.join(data_path, 'group2.int_valid.p'), 'rb'))\n",
        "int_test = pickle.load(open(os.path.join(data_path, 'group2.int_test.p'), 'rb'))\n",
        "vocab_dict = pickle.load(open(os.path.join(data_path, 'group2.vocab_dict.p'), 'rb'))\n",
        "\n",
        "train_batches_int = gen_batches(5, 30, int_train)\n",
        "val_batches_int = gen_batches(5, 30, int_validation)\n",
        "test_batches_int = gen_batches(5, 30, int_test)\n",
        "\n",
        "run01_params = BengioParams(vocab_dict=vocab_dict)\n",
        "run01_params.embedding_dimensions = 100\n",
        "run01_params.hidden_units = 100\n",
        "run01_params.context_window = 5\n",
        "\n",
        "run01_model = BengioModel2(params=run01_params)\n",
        "\n",
        "run01_perplexity_history,  run01_cost_history, run01_val_perplexity_history, run01_val_cost_history, run01_test_perplexity_history, run01_test_cost_history = run(run01_model, run01_params, train_batches_int, val_batches_int, test_batches_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SFcnpqwBpam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}